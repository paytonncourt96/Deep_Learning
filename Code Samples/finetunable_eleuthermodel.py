# -*- coding: utf-8 -*-
"""FineTunable EleutherModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t2k4ZTO3HwnJ-H6CaqZk9E2KOQIvIWZ8

**Make Sure to connect to A100 GPU**
Uncomment belo to instyall dependences
"""

#!pip install --upgrade fsspec
#!pip install --upgrade torch transformers datasets sentence-transformers fsspec gcsfs
!pip install faiss-gpu

import sentence_transformers
import transformers
import faiss
import torch
import datasets
import huggingface_hub
import torch
print("CUDA available:", torch.cuda.is_available())
print("Current device:", torch.cuda.current_device())
print("Device name:", torch.cuda.get_device_name(0))


print(f"Sentence Transformers version: {sentence_transformers.__version__}")
print(f"Hugging Face Hub version: {huggingface_hub.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"FAISS version: {faiss.__version__}")
print(f"Torch version: {torch.__version__}")
print(f"Datasets version: {datasets.__version__}")

import json
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset
from datasets import Dataset
from torch.utils.data import DataLoader
from sentence_transformers import InputExample, losses
file = "/content/drive/MyDrive/Deep_Learning/Project/biomarker_terms.json"
# Load JSON
with open(file, "r") as file:
    data = json.load(file)


# Parse JSON to Extract Relevant Data
def parse_nodes(json_data):
    """Extracts relevant fields from the 'nodes' dictionary."""
    nodes = json_data.get("nodes", {})
    parsed_data = []
    for node_id, node_details in nodes.items():
        label = node_details.get("label", "")
        parent = node_details.get("parent", "")
        children = ", ".join(node_details.get("children", []))
        synonyms = ", ".join(node_details.get("synonymLabels", []))
        parsed_data.append({
            "id": node_id,
            "label": label,
            "parent": parent,
            "children": children,
            "synonyms": synonyms
        })
    return pd.DataFrame(parsed_data)

# Flatten
df = parse_nodes(data)

# put into text
df['combined_text'] = df.apply(
    lambda row: f"Label: {row['label']}, Parent: {row['parent']}, "
                f"Children: {row['children']}, Synonyms: {row['synonyms']}",
    axis=1
)

from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import DataLoader

# train data
train_data = [
    InputExample(texts=["What is ABCA1?", "ABCA1 is a gene that is part of the biomarker structure with no children, synonymLabels and can be found at http://identifiers.org/hgnc/29."]),
    InputExample(texts=["Tell me about smoothelin.", "smoothelin is a gene with synonymLabels SMTN, has no children, and can be found at http://identifiers.org/hgnc/11126."]),
]

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)

#loss
train_loss = losses.MultipleNegativesRankingLoss(embedding_model)

# Fine-tune the model
embedding_model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,
    output_path="fine_tuned_model",
    show_progress_bar=True
)
#api key
#Create Hugging Face Account to generate API Key

embedding_model = SentenceTransformer("fine_tuned_model")
df['embeddings'] = df['combined_text'].apply(lambda text: embedding_model.encode(text))
embeddings = np.vstack(df['embeddings'].to_numpy())

# Create FAISS Index for Retrieval
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)
text_data = df['combined_text'].to_list()

fine_tune_data = [
    {"prompt": "Context: ABCA1 is a gene.\n\nQuestion: What is ABCA1?\nAnswer:",
     "completion": "ABCA1 is a gene that is part of the biomarker structure with no children, synonymLabels and can be found at http://identifiers.org/hgnc/29."},
    {"prompt": "Context: ABCA3 has no children and synonymLabels SMTN.\n\nQuestion: Tell me about smoothelin.\nAnswer:",
     "completion": "smoothelin is a gene with synonymLabels SMTN, has no children, and can be found at http://identifiers.org/hgnc/11126."},
]
with open("fine_tune_data.jsonl", "w") as f:
    for item in fine_tune_data:
        f.write(json.dumps(item) + "\n")

dataset = load_dataset('json', data_files={'train': 'fine_tune_data.jsonl'})['train']

#tokens
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer.pad_token = tokenizer.eos_token
def preprocess_function(examples):
    return tokenizer(
        examples["prompt"],
        text_target=examples["completion"],
        truncation=True,
        padding="max_length",
        max_length=512
    )

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["prompt", "completion"])

"""Fine Tune"""

training_args = TrainingArguments(
    output_dir="./fine_tuned_gpt_neo",
    evaluation_strategy="no",
    learning_rate=5e-5,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    weight_decay=0.01,
    save_steps=10_000,
    save_total_limit=2,
)

gpt_model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
trainer = Trainer(
    model=gpt_model,
    args=training_args,
    train_dataset=tokenized_dataset,
)
trainer.train()

gpt_model.save_pretrained("./fine_tuned_gpt_neo")

def retrieve_context(question, k=5):
    """Retrieve the most relevant context for a given question."""
    embedding_model.to("cuda")  # Ensure embedding model is on GPU
    question_embedding = embedding_model.encode(question)
    question_embedding = np.array([question_embedding])  # FAISS requires NumPy
    distances, indices = index.search(question_embedding, k)
    # Filter out empty or irrelevant contexts
    relevant_contexts = [text_data[i] for i in indices[0] if "smoothelin" in text_data[i].lower()]
    return relevant_contexts

def generate_response_with_context(question, context):
    """Generate a response using GPT-Neo that explicitly involves the context."""
    prompt = (
    f"The document provides the following information:\n{context}\n\n"
    f"Based on the above, here is what I know about {question.split()[-1]}:")

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda")

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    attention_mask = input_ids != tokenizer.pad_token_id
    output = gpt_model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_length=400,
        num_return_sequences=1,
        temperature=1.0,
        do_sample=True
    )
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print("Raw model output:", response)  # Debugging
    return response


def answer_question_with_context(question):
    """Answer a question by retrieving context and generating a context-based response."""
    retrieved_context = " ".join(retrieve_context(question))
    print("Retrieved context:", retrieved_context)  # Debugging
    if not retrieved_context.strip():
        return "No relevant context found for the given question."
    response = generate_response_with_context(question, retrieved_context)
    return response


# Example Usage
question = "What can you tell me about smoothelin?"
print(answer_question_with_context(question))
